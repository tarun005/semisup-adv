2020-06-10 00:40:42,859 | Robust self-training
2020-06-10 00:40:42,859 | Args: Namespace(autoaugment=False, aux_data_filename=None, aux_take_amount=None, batch_size=300, beta=6.0, cutout=False, data_dir='/newfoundland/tarun/datasets/Digits/SVHN/', dataset='svhn', distance='l_inf', entropy_weight=0.0, epochs=120, epsilon=0.031, eval_attack_batches=1, eval_freq=4, log_interval=5, loss='trades', lr=0.1, lr_schedule='cosine', model='wrn-40-2', model_dir='svhn_aug_5p_noise_RST_adv', momentum=0.9, nesterov=True, normalize_input=False, overwrite=False, pgd_num_steps=10, pgd_step_size=0.007, remove_pseudo_labels=False, save_freq=25, seed=1, svhn_extra=True, test_batch_size=512, train_eval_batches=None, unsup_fraction=0.5, weight_decay=0.0005)
2020-06-10 00:40:46,490 | --Training set--
2020-06-10 00:40:46,491 | Number of training samples: 604388
2020-06-10 00:40:46,491 | Number of supervised samples: 604388
2020-06-10 00:40:46,492 | Number of unsup samples: 0
2020-06-10 00:40:46,549 | Label (and pseudo-label) histogram: ((0, 50686), (1, 103311), (2, 84348), (3, 68382), (4, 58104), (5, 60362), (6, 48010), (7, 50013), (8, 41218), (9, 39954))
2020-06-10 00:40:46,549 | Shape of training data: (604388, 3, 32, 32)
2020-06-10 00:40:47,177 | --Test set--
2020-06-10 00:40:47,177 | Number of samples: 26032
2020-06-10 00:40:47,179 | Label histogram: ((0, 1744), (1, 5099), (2, 4149), (3, 2882), (4, 2523), (5, 2384), (6, 1977), (7, 2019), (8, 1660), (9, 1595))
2020-06-10 00:40:47,180 | Shape of data: (26032, 3, 32, 32)
2020-06-10 00:40:50,588 | --Training set--
2020-06-10 00:40:50,588 | Number of training samples: 604388
2020-06-10 00:40:50,588 | Number of supervised samples: 604388
2020-06-10 00:40:50,588 | Number of unsup samples: 0
2020-06-10 00:40:50,644 | Label (and pseudo-label) histogram: ((0, 50686), (1, 103311), (2, 84348), (3, 68382), (4, 58104), (5, 60362), (6, 48010), (7, 50013), (8, 41218), (9, 39954))
2020-06-10 00:40:50,645 | Shape of training data: (604388, 3, 32, 32)
2020-06-10 00:40:54,560 | Setting learning rate to 0.1
2020-06-10 00:41:19,452 | Robust self-training
2020-06-10 00:41:19,452 | Args: Namespace(autoaugment=False, aux_data_filename=None, aux_take_amount=None, batch_size=300, beta=6.0, cutout=False, data_dir='/newfoundland/tarun/datasets/Digits/SVHN/', dataset='svhn', distance='l_inf', entropy_weight=0.0, epochs=120, epsilon=0.031, eval_attack_batches=1, eval_freq=4, log_interval=5, loss='trades', lr=0.1, lr_schedule='cosine', model='wrn-40-2', model_dir='svhn_aug_5p_noise_RST_adv', momentum=0.9, nesterov=True, normalize_input=False, overwrite=False, pgd_num_steps=10, pgd_step_size=0.007, remove_pseudo_labels=False, save_freq=25, seed=1, svhn_extra=True, test_batch_size=512, train_eval_batches=None, unsup_fraction=0.5, weight_decay=0.0005)
2020-06-10 00:41:23,060 | --Training set--
2020-06-10 00:41:23,060 | Number of training samples: 604388
2020-06-10 00:41:23,061 | Number of supervised samples: 604388
2020-06-10 00:41:23,061 | Number of unsup samples: 0
2020-06-10 00:41:23,119 | Label (and pseudo-label) histogram: ((0, 50686), (1, 103311), (2, 84348), (3, 68382), (4, 58104), (5, 60362), (6, 48010), (7, 50013), (8, 41218), (9, 39954))
2020-06-10 00:41:23,119 | Shape of training data: (604388, 3, 32, 32)
2020-06-10 00:41:23,740 | --Test set--
2020-06-10 00:41:23,740 | Number of samples: 26032
2020-06-10 00:41:23,742 | Label histogram: ((0, 1744), (1, 5099), (2, 4149), (3, 2882), (4, 2523), (5, 2384), (6, 1977), (7, 2019), (8, 1660), (9, 1595))
2020-06-10 00:41:23,742 | Shape of data: (26032, 3, 32, 32)
2020-06-10 00:41:27,118 | --Training set--
2020-06-10 00:41:27,118 | Number of training samples: 604388
2020-06-10 00:41:27,118 | Number of supervised samples: 604388
2020-06-10 00:41:27,118 | Number of unsup samples: 0
2020-06-10 00:41:27,174 | Label (and pseudo-label) histogram: ((0, 50686), (1, 103311), (2, 84348), (3, 68382), (4, 58104), (5, 60362), (6, 48010), (7, 50013), (8, 41218), (9, 39954))
2020-06-10 00:41:27,174 | Shape of training data: (604388, 3, 32, 32)
2020-06-10 00:41:29,229 | Setting learning rate to 0.1
2020-06-10 00:41:32,801 | Train Epoch: 1 [0/604500 (0%)]	Loss: 2.352771
2020-06-10 00:41:38,690 | Train Epoch: 1 [1500/604500 (0%)]	Loss: 2.271848
2020-06-10 00:41:44,552 | Train Epoch: 1 [3000/604500 (0%)]	Loss: 2.247132
2020-06-10 00:41:50,446 | Train Epoch: 1 [4500/604500 (1%)]	Loss: 2.232259
2020-06-10 00:41:56,327 | Train Epoch: 1 [6000/604500 (1%)]	Loss: 2.240022
2020-06-10 00:42:02,228 | Train Epoch: 1 [7500/604500 (1%)]	Loss: 2.203570
2020-06-10 00:42:08,118 | Train Epoch: 1 [9000/604500 (1%)]	Loss: 2.157998
2020-06-10 00:42:14,018 | Train Epoch: 1 [10500/604500 (2%)]	Loss: 2.139435
2020-06-10 00:42:19,919 | Train Epoch: 1 [12000/604500 (2%)]	Loss: 2.165728
2020-06-10 00:42:25,850 | Train Epoch: 1 [13500/604500 (2%)]	Loss: 2.173100
2020-06-10 00:42:31,762 | Train Epoch: 1 [15000/604500 (2%)]	Loss: 2.156206
2020-06-10 00:42:37,685 | Train Epoch: 1 [16500/604500 (3%)]	Loss: 2.134859
2020-06-10 00:42:43,603 | Train Epoch: 1 [18000/604500 (3%)]	Loss: 2.134527
2020-06-10 00:42:49,520 | Train Epoch: 1 [19500/604500 (3%)]	Loss: 2.034232
2020-06-10 00:42:55,439 | Train Epoch: 1 [21000/604500 (3%)]	Loss: 1.978645
2020-06-10 00:43:01,358 | Train Epoch: 1 [22500/604500 (4%)]	Loss: 1.953701
2020-06-10 00:43:07,277 | Train Epoch: 1 [24000/604500 (4%)]	Loss: 1.925167
2020-06-10 00:43:13,198 | Train Epoch: 1 [25500/604500 (4%)]	Loss: 1.879556
2020-06-10 00:43:19,120 | Train Epoch: 1 [27000/604500 (4%)]	Loss: 1.813063
2020-06-10 00:43:25,042 | Train Epoch: 1 [28500/604500 (5%)]	Loss: 1.888282
2020-06-10 00:43:30,967 | Train Epoch: 1 [30000/604500 (5%)]	Loss: 1.799777
2020-06-10 00:43:36,896 | Train Epoch: 1 [31500/604500 (5%)]	Loss: 1.762104
2020-06-10 00:43:42,825 | Train Epoch: 1 [33000/604500 (5%)]	Loss: 1.735852
2020-06-10 00:43:48,748 | Train Epoch: 1 [34500/604500 (6%)]	Loss: 1.715099
2020-06-10 00:43:54,673 | Train Epoch: 1 [36000/604500 (6%)]	Loss: 1.664301
2020-06-10 00:44:00,595 | Train Epoch: 1 [37500/604500 (6%)]	Loss: 1.731171
2020-06-10 00:44:06,516 | Train Epoch: 1 [39000/604500 (6%)]	Loss: 1.663344
2020-06-10 00:44:12,443 | Train Epoch: 1 [40500/604500 (7%)]	Loss: 1.600618
2020-06-10 00:44:18,363 | Train Epoch: 1 [42000/604500 (7%)]	Loss: 1.625377
2020-06-10 00:44:24,304 | Train Epoch: 1 [43500/604500 (7%)]	Loss: 1.622208
2020-06-10 00:44:30,236 | Train Epoch: 1 [45000/604500 (7%)]	Loss: 1.679126
2020-06-10 00:44:36,162 | Train Epoch: 1 [46500/604500 (8%)]	Loss: 1.535781
2020-06-10 00:44:42,087 | Train Epoch: 1 [48000/604500 (8%)]	Loss: 1.440202
2020-06-10 00:44:48,010 | Train Epoch: 1 [49500/604500 (8%)]	Loss: 1.531359
2020-06-10 00:44:53,938 | Train Epoch: 1 [51000/604500 (8%)]	Loss: 1.510604
2020-06-10 00:44:59,874 | Train Epoch: 1 [52500/604500 (9%)]	Loss: 1.488154
2020-06-10 00:45:05,810 | Train Epoch: 1 [54000/604500 (9%)]	Loss: 1.559799
2020-06-10 00:45:11,755 | Train Epoch: 1 [55500/604500 (9%)]	Loss: 1.520141
2020-06-10 00:45:17,693 | Train Epoch: 1 [57000/604500 (9%)]	Loss: 1.574315
2020-06-10 00:45:23,632 | Train Epoch: 1 [58500/604500 (10%)]	Loss: 1.548889
2020-06-10 00:45:29,570 | Train Epoch: 1 [60000/604500 (10%)]	Loss: 1.483871
2020-06-10 00:45:35,511 | Train Epoch: 1 [61500/604500 (10%)]	Loss: 1.452630
2020-06-10 00:45:41,453 | Train Epoch: 1 [63000/604500 (10%)]	Loss: 1.462740
2020-06-10 00:45:47,394 | Train Epoch: 1 [64500/604500 (11%)]	Loss: 1.543709
