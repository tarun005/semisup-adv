2020-06-10 00:43:02,122 | Robust self-training
2020-06-10 00:43:02,122 | Args: Namespace(autoaugment=False, aux_data_filename=None, aux_take_amount=None, batch_size=300, beta=6.0, cutout=False, data_dir='/newfoundland/tarun/datasets/Digits/SVHN/', dataset='svhn', distance='l_inf', entropy_weight=0.0, epochs=120, epsilon=0.031, eval_attack_batches=1, eval_freq=4, log_interval=5, loss='trades', lr=0.1, lr_schedule='cosine', model='wrn-40-2', model_dir='svhn_aug_10p_noise_RST_adv', momentum=0.9, nesterov=True, normalize_input=False, overwrite=False, pgd_num_steps=10, pgd_step_size=0.007, remove_pseudo_labels=False, save_freq=25, seed=1, svhn_extra=True, test_batch_size=512, train_eval_batches=None, unsup_fraction=0.5, weight_decay=0.0005)
2020-06-10 00:43:05,816 | --Training set--
2020-06-10 00:43:05,816 | Number of training samples: 604388
2020-06-10 00:43:05,817 | Number of supervised samples: 604388
2020-06-10 00:43:05,817 | Number of unsup samples: 0
2020-06-10 00:43:05,875 | Label (and pseudo-label) histogram: ((0, 50854), (1, 101242), (2, 83257), (3, 68096), (4, 58299), (5, 60499), (6, 48652), (7, 50443), (8, 42154), (9, 40892))
2020-06-10 00:43:05,876 | Shape of training data: (604388, 3, 32, 32)
2020-06-10 00:43:06,529 | --Test set--
2020-06-10 00:43:06,529 | Number of samples: 26032
2020-06-10 00:43:06,532 | Label histogram: ((0, 1744), (1, 5099), (2, 4149), (3, 2882), (4, 2523), (5, 2384), (6, 1977), (7, 2019), (8, 1660), (9, 1595))
2020-06-10 00:43:06,532 | Shape of data: (26032, 3, 32, 32)
2020-06-10 00:43:10,172 | --Training set--
2020-06-10 00:43:10,172 | Number of training samples: 604388
2020-06-10 00:43:10,172 | Number of supervised samples: 604388
2020-06-10 00:43:10,173 | Number of unsup samples: 0
2020-06-10 00:43:10,230 | Label (and pseudo-label) histogram: ((0, 50854), (1, 101242), (2, 83257), (3, 68096), (4, 58299), (5, 60499), (6, 48652), (7, 50443), (8, 42154), (9, 40892))
2020-06-10 00:43:10,230 | Shape of training data: (604388, 3, 32, 32)
2020-06-10 00:43:12,452 | Setting learning rate to 0.1
2020-06-10 00:43:16,127 | Train Epoch: 1 [0/604500 (0%)]	Loss: 2.359977
2020-06-10 00:43:21,871 | Train Epoch: 1 [1500/604500 (0%)]	Loss: 2.266241
2020-06-10 00:43:27,618 | Train Epoch: 1 [3000/604500 (0%)]	Loss: 2.243598
2020-06-10 00:43:33,385 | Train Epoch: 1 [4500/604500 (1%)]	Loss: 2.243424
2020-06-10 00:43:39,162 | Train Epoch: 1 [6000/604500 (1%)]	Loss: 2.254828
2020-06-10 00:43:44,945 | Train Epoch: 1 [7500/604500 (1%)]	Loss: 2.244576
2020-06-10 00:43:50,732 | Train Epoch: 1 [9000/604500 (1%)]	Loss: 2.231091
2020-06-10 00:43:56,523 | Train Epoch: 1 [10500/604500 (2%)]	Loss: 2.185716
2020-06-10 00:44:02,320 | Train Epoch: 1 [12000/604500 (2%)]	Loss: 2.160061
2020-06-10 00:44:08,109 | Train Epoch: 1 [13500/604500 (2%)]	Loss: 2.197263
2020-06-10 00:44:13,903 | Train Epoch: 1 [15000/604500 (2%)]	Loss: 2.193681
2020-06-10 00:44:19,711 | Train Epoch: 1 [16500/604500 (3%)]	Loss: 2.192621
2020-06-10 00:44:25,523 | Train Epoch: 1 [18000/604500 (3%)]	Loss: 2.147043
2020-06-10 00:44:31,337 | Train Epoch: 1 [19500/604500 (3%)]	Loss: 2.095822
2020-06-10 00:44:37,151 | Train Epoch: 1 [21000/604500 (3%)]	Loss: 2.050670
2020-06-10 00:44:42,965 | Train Epoch: 1 [22500/604500 (4%)]	Loss: 1.985001
2020-06-10 00:44:48,782 | Train Epoch: 1 [24000/604500 (4%)]	Loss: 1.987498
2020-06-10 00:44:54,600 | Train Epoch: 1 [25500/604500 (4%)]	Loss: 1.907022
2020-06-10 00:45:00,426 | Train Epoch: 1 [27000/604500 (4%)]	Loss: 1.887443
2020-06-10 00:45:06,252 | Train Epoch: 1 [28500/604500 (5%)]	Loss: 1.912508
2020-06-10 00:45:12,074 | Train Epoch: 1 [30000/604500 (5%)]	Loss: 1.864572
2020-06-10 00:45:17,897 | Train Epoch: 1 [31500/604500 (5%)]	Loss: 1.818246
2020-06-10 00:45:23,717 | Train Epoch: 1 [33000/604500 (5%)]	Loss: 1.884664
2020-06-10 00:45:29,543 | Train Epoch: 1 [34500/604500 (6%)]	Loss: 1.747213
2020-06-10 00:45:35,365 | Train Epoch: 1 [36000/604500 (6%)]	Loss: 1.726912
2020-06-10 00:45:41,189 | Train Epoch: 1 [37500/604500 (6%)]	Loss: 1.687611
2020-06-10 00:45:47,013 | Train Epoch: 1 [39000/604500 (6%)]	Loss: 1.739931
